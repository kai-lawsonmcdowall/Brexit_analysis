---
title: "Assessed Pratical 2 201482038"
author: "Kai Lawson-McDowall"
date: "25/03/2021"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---
 
                      

1. (a) Fit a logistic regression model for the outputs using all of the available inputs. Explain your model and report your results. Identify the direction and magnitude of each effect from the fitted coefficients. Compare these with the plots shown on the Guardian website. Do your findings agree with these plots? Comment on your findings.

I created a logistic regression model attempts to predict an individuals voting choice for brexit ("voteBrexit") using the following input variables from the dataframe "brexit":

          -"abc1": proportion of individuals who are in the ABC1 social classes (middle to upper class)
          -"medianIncome": the median income of all residents
          -"medianAge": median age of residents
          -"withHigherEd": proportion of residents with any university-level education
          -"notBornUK": the proportion of residents who were born outside the UK

As logistic regression is a special case of generalized linear models, the glm() function was used to create this model. Although logistic regression models such as this are non-linear, the glm() function implements a description of error distribution and the link function to be used, in this case,as the outcome is binary, the "binomial" function is used.  

When considering the magnitude and direction of the coefficients, negative coefficients indicate there is an inverse relationship between an increase in a particular variable and voting "yes" to Brexit, whereas positive coefficients indicate a direct relationship between an increase in the variable and voting "yes" to Brexit. When comparing the direction and magnitude of each of these variables, they do seem to align with the plots on the guardian website. Higher education, which had the strongest inverse relationship with a "yes" vote in our regression is also reflected in the Guardians plot, this was also the case in our model and the guardian plot for median income, despite the magnitude of median income being smaller (-26.7443 vs. -6.3857).

In terms of explanatory variables with direct relationships, the median age also agrees with the guardian plot, which showed areas with increasing older populations also voted leave more frequently. 

However, the coefficients for the proportion of individuals who are in the ABC1 social classes and the proportion of residents who were born outside the UK are inconsistent with the guardian plots. For notbornUK, there appears to be a direct relationship, although there is a wide spread of data in the guardian plot, it still seems that areas with greater percentages of residents not born in the UK had fewer votes to leave. For abc1, there was a strong positive coefficient indicating that an increase in an areas abc1 population were more likely to vote "leave". The guardian plot contradicts this, showing populations with a large percent of ABC1 grades were voted remain more often. 

```{r include=FALSE, echo = FALSE, warning=FALSE, message=FALSE, results='hide',collapse=TRUE, }
library(data.table)
brexit=fread('C:\\Users\\Kai\\Desktop\\Statistical learning\\Assessed Practical 2\\brexit.csv')
brexit
```


```{r echo=FALSE, warning=FALSE}

as.data.frame(brexit)
brexitglm <- glm(formula = voteBrexit~abc1+notBornUK+medianIncome+medianAge+withHigherEd,  family = binomial, data = brexit) #performing the logistic regression
summary(brexitglm)
```

(b) Present the value of each coefficient estimate with a 95% confidence interval. Which inputs would you say have strong      effects? Order the inputs in terms of decreasing effect. Comment on your findings and justify your reasoning.

In logistic regression, the standard errors are approximated by assuming that the likelihood is normally distributed, meaning the CI's are  approximate, therefore, we assume that the data is large and use the appropriate value from the normal distribution (z-distribution). The variables, ranked from strongest to weakest effect were: 

      Higher education:        -26.74    (95 CI: -33.75 to -19.74)
      Middle to upper class:    17.58    (95 CI: 11.87 to 23.28)
      Median income:           - 6.39    (95 CI: -10.15 to -2.62)
      Median age:                5.92    (95 CI: 3.16 to 8.68)
      Not born in the UK:        5.69    (95 CI: 2.15 to 9.22)
      
with the following ranking (magnitudes): 

withHigherEd         abc1 medianIncome    medianAge    notBornUK  (Intercept) 
  26.7442592   17.5779980    6.3857396    5.9208767    5.6861383    0.1385963 


The variables with "strong" effects are those with a greater magnitude, or absolute value of their estimate. For instance, although higher education status is negative, its absolute value is 26.74, and so an increase in higher education has the strongest effect on whether or not someone voted, irrespective of its direction. When interpreting the results, higher education having the strongest effect is somewhat expected. For example, the guardian article mentions that the best predictor of a vote for remain is the proportion of residents who have a degree. It also also worth noting that although Median age appears to be a stronger predictor than Not born in the UK, their confidence intervals overlap, meaning that Not born in the UK could potentially be a better predictor. 

```{r include=FALSE, warning=FALSE}

zc = qnorm(0.975) #Get critical value of z at 2.5% (should be 1.96)

#Extract estimate and standard error of coefficient from model summary and get the 95% CIs 

#middle to upper class
abc_estimate = summary(brexitglm)$coefficients[2,1]
abc_standard_error = summary(brexitglm)$coefficients[2,2]
abc_min = abc_estimate - zc*abc_standard_error
abc_max = abc_estimate + zc*abc_standard_error

#not born in the UK 
notBornUK_estimate = summary(brexitglm)$coefficients[3,1]
notBornUK_standard_error = summary(brexitglm)$coefficients[3,2]
notBornUK_min = notBornUK_estimate - zc*notBornUK_standard_error
notBornUK_max = notBornUK_estimate + zc*notBornUK_standard_error

#median income
medianIncome_estimate = summary(brexitglm)$coefficients[4,1]
medianIncome_standard_error = summary(brexitglm)$coefficients[4,2]
medianIncome_min = medianIncome_estimate - zc*medianIncome_standard_error
medianIncome_max = medianIncome_estimate + zc*medianIncome_standard_error

#median age
medianAge_estimate = summary(brexitglm)$coefficients[5,1]
medianAge_standard_error = summary(brexitglm)$coefficients[5,2]
medianAge_min = medianAge_estimate - zc*medianAge_standard_error
medianAge_max = medianAge_estimate + zc*medianAge_standard_error

#higher education status
withHigherEd_estimate = summary(brexitglm)$coefficients[6,1]
WithHigherEd_standard_error = summary(brexitglm)$coefficients[6,2]
WithHigherEd_min = withHigherEd_estimate - zc*WithHigherEd_standard_error
WithHigherEd_max = withHigherEd_estimate + zc*WithHigherEd_standard_error

#confirming which estimates from largest to smallest effect

sort(abs(summary(brexitglm)$coefficient[,1]), decreasing = TRUE) # Estimate and 95 CI's (ordered from largest to smallest effect )

sprintf("higher education: %e (95 CI: %f to %g)",withHigherEd_estimate, WithHigherEd_min,WithHigherEd_max)
sprintf("Middle to upper class: %e (95 CI: %f to %g)",abc_estimate, abc_min,abc_max)
sprintf("Median income: %e (95 CI: %f to %g)", medianIncome_estimate, medianIncome_min,medianIncome_max)
sprintf("Median age: %e (95 CI: %f to %g)",medianAge_estimate, medianAge_min, medianAge_max)
sprintf("Not born in the UK: %e (95 CI: %f to %g)",notBornUK_estimate, notBornUK_min,notBornUK_max)

```



(c) Using aic, perform model selection to determine which factors are useful to predict the result of
the vote. Use a ‘greedy’ input selection procedure, as follows: (i) select the best model with 1
input; (ii) fixing that input, select the best two-input model (i.e. try all the other 4 inputs with the
one you selected first); (iii) select the best three-input model containing the first two inputs you
chose, etc. At each stage evaluate the quality of fit using aic and stop if this gets worse. Report
your results and comment on your findings. Are your findings consistent with the Task 1.(b)?

In order to perform model selection to determine which factors are useful in predicting the result of the vote, I used the "MuMIn" package. This allowed me to create models using all possible combinations of the 5 explanatory variables (32 models in total) and perform greedy selection by simply extracting AIC values from the table produced. From this, the best model with 1 input was the model using the Higher education status (AIC = 313.5956). Fixing this, the model containing 2 inputs was Higher Education + abc1 (AIC = 286.6160). Following this pattern, the 3-parameter model was Higher Education, abc1, and Median Age (AIC = 272.0497) and the 4 Parameter model, was Higher Education, abc1, Median Age, and median income. The model with all the input had the lowest AIC (259.6344). It is notable how the AIC continues to decrease even up to 5 parameters, however, these inputs were likely identified first place as having a meaningful relationship with how people voted for Brexit, meaning this result isn't as surprising as it initially seemed. 

The results of greedy input selection somewhat reflect the results of 1b. Firstly, the best-one input model used higher education status, which has the strongest effect as seen in 1b. The two-input model used higher education and income, which have the 1st and 2nd largest effects respectively. The 3 input model deviates from 1b slightly, if we were expecting additional inputs to be based on the strength of their individual effects alone, then based on the results from 1b, we would expect the next input to be median Income, instead, it is median age. However, median income is included in the 4-input model, meaning the 4-input model from the greedy selection is consistent with the results in 1b.

```{r include=FALSE, warning=FALSE}
# Mumin, creates all models with all possible combination of variables and selected the models from the table produced: 

library(MuMIn)
allvariables <- glm(voteBrexit~., family=binomial(), na.action = "na.fail", data=brexit)
dd <-dredge(allvariables)
dd <-as.data.frame(dd) #makes it easier to manipulate and order 
dd


```


2. Use the rpart package to create a decision tree classification model. Explain and visualize your model
and interpret the fitted model.

The top node of the desicion tree checks if higher education>= 0.47, if it is then we assume that they voted to remain (FALSE). If it lower than 0.47, we then check if their notBornUK status is >= 0.43. If it is, we conclude they voted to remain.  Otherwise, we look at higher education again, if less than 0.31, we assume true (Voted to leave). Finally, we determine if median is income >= 0.41, if it is, then we assume they voted to remain and if <0.41 conclude they voted to leave.  Although this model is arguably more intuitive than the logistic regression model we have used so far, it does consider all of the input variables in the Brexit dataset. 

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(fields)
library(rpart)
library(rpart.plot)
brexittree = rpart (voteBrexit ~ ., data = brexit, method = 'class')
prp(brexittree)

#just a probability field out of interest. 
prediction = predict(brexittree, newdata = brexit)
prediction = prediction[,2]
#fields::image.plot(matrix(prediction, 20,20))


#echo=FALSE #prevents the code showing 
#results="hide" # prevents the results printing 


```

3. Compare your decision tree model and your logistic regression model. Do they attribute high importance to the same factors? Interpret each model to explain the referendum vote.

To some degree, these models attribute importance to the same factors. For instance, two of the decision nodes in the decision tree rely on higher education status, which as mentioned displays the strongest effect in the logistic regression model. The decision tree also uses abc1 status as a decision node, which displayed the 2nd strongest effect in the  regression model. However, the decision tree also uses notbornUK as a decision node, which ranks as the input with the smallest effect in the regression model. In parallel, the desicion tree model doesn't appear to place any value on median income or median age in desicion-making process. 

In terms of how each model works to make a prediction about the referendum, the desicion tree bisect the space into smaller and smaller regions, whereas our Logistic Regression model fits a line (or equivalent in the 5-dimensional space we have in our model). A key aspect of our logistic regression model is that is also assumes that the probability of a true output (voted to leave) compared to false (voted to remain) changes monotonically  as the inputs decrease or increase, which means that the logistic regression model use of a single desicion boundary is good at capturing gradually changing functions. Conversely, our decision tree's partitioning of the the input space into distinct areas means they are better suited for single partition (e.g. step wise functions, such as the top hat function mentioned). Determining which is better for predicting the outcome could be approached with methods such as cross validation. 



4. Which model would you use if you were explaining the results for a newspaper article, and why?

If I were explaining the results in a newspaper article, my goal would arguably be to communicate as much important information to the general public as simply as possible. Although the logistic regression models may be better at modeling how each of the explanatory variables contributes to how an individual voted (e.g. less chance of overfitting), for someone who does not have a strong knowledge of statistics, they would have trouble using this to gain meaningful insight about how people actually voted. For this reason, I would use the decision tree, as it provides an intuitive and understandable result. That being said, decision trees in isolation are often unstable, with small changes in the data leading to a large change in their structures, as well as poor predictors. Although their poor predictive power could be rectified through a random forest, this again creates an issue of interpretabilty. It is also worth mentioning that although this decision tree is easier to interpret than the logistic regression model, the normalized values also hinder interpretability (i.e. a higher education status of >= 0.47 has no real meaning to the reader) and so could be changed for publication in the newspaper. 



                                          R code: 


1) a

```{R eval=FALSE}

as.data.frame(brexit)
brexitglm <- glm(formula = voteBrexit~abc1+notBornUK+medianIncome+medianAge+withHigherEd,  
                 family = binomial, data = brexit) #performing the logistic regression
summary(brexitglm)

```


1) b
```{r eval=FALSE}

zc = qnorm(0.975) #Get critical value of z at 2.5% (should be 1.96)

#Extract estimate and standard error of coefficient from model summary and get the 95% CIs 

#middle to upper class
abc_estimate = summary(brexitglm)$coefficients[2,1]
abc_standard_error = summary(brexitglm)$coefficients[2,2]
abc_min = abc_estimate - zc*abc_standard_error
abc_max = abc_estimate + zc*abc_standard_error

#not born in the UK 
notBornUK_estimate = summary(brexitglm)$coefficients[3,1]
notBornUK_standard_error = summary(brexitglm)$coefficients[3,2]
notBornUK_min = notBornUK_estimate - zc*notBornUK_standard_error
notBornUK_max = notBornUK_estimate + zc*notBornUK_standard_error

#median income
medianIncome_estimate = summary(brexitglm)$coefficients[4,1]
medianIncome_standard_error = summary(brexitglm)$coefficients[4,2]
medianIncome_min = medianIncome_estimate - zc*medianIncome_standard_error
medianIncome_max = medianIncome_estimate + zc*medianIncome_standard_error

#median age
medianAge_estimate = summary(brexitglm)$coefficients[5,1]
medianAge_standard_error = summary(brexitglm)$coefficients[5,2]
medianAge_min = medianAge_estimate - zc*medianAge_standard_error
medianAge_max = medianAge_estimate + zc*medianAge_standard_error

#higher education status
withHigherEd_estimate = summary(brexitglm)$coefficients[6,1]
WithHigherEd_standard_error = summary(brexitglm)$coefficients[6,2]
WithHigherEd_min = withHigherEd_estimate - zc*WithHigherEd_standard_error
WithHigherEd_max = withHigherEd_estimate + zc*WithHigherEd_standard_error

#confirming which estimates from largest to smallest effect

sort(abs(summary(brexitglm)$coefficient[,1]), decreasing = TRUE) # Estimate and 95 CI's (ordered from largest to smallest effect )

sprintf("higher education: %e (95 CI: %f to %g)",
        withHigherEd_estimate, WithHigherEd_min,WithHigherEd_max)
sprintf("Middle to upper class: %e (95 CI: %f to %g)",
        abc_estimate, abc_min,abc_max)
sprintf("Median income: %e (95 CI: %f to %g)", 
        medianIncome_estimate, medianIncome_min,medianIncome_max)
sprintf("Median age: %e (95 CI: %f to %g)",
        medianAge_estimate, medianAge_min, medianAge_max)
sprintf("Not born in the UK: %e (95 CI: %f to %g)",
        notBornUK_estimate, notBornUK_min,notBornUK_max)

```


1) c
```{r eval=FALSE}

library(MuMIn) # creates all possible models and puts into table
allvariables <- glm(voteBrexit~., family=binomial(), na.action = "na.fail", data=brexit)
dd <-dredge(allvariables)
dd <-as.data.frame(dd) #makes it easier to manipulate and order 
dd


```


2)
```{r eval=FALSE}

library(fields)
library(rpart)
library(rpart.plot)
brexittree = rpart (voteBrexit ~ ., data = brexit, method = 'class')
prp(brexittree)

#just a probability field out of interest. 
prediction = predict(brexittree, newdata = brexit)
prediction = prediction[,2]
#fields::image.plot(matrix(prediction, 20,20))


#echo=FALSE #prevents the code showing 
#results="hide" # prevents the results printing 
```
